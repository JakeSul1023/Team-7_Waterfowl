# Load libraries
library(sp)
library(terra)
version
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- Mallard_Connectivity_Recent_Data
# Step 2: Inspect Dataset Structure
str(waterfowl_data)
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- Mallard_Connectivity_Recent_Data
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- Mallard_Connectivity_Recent_Data.csv
install.packages("readr")
library(readr)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- read_csv("Mallard_Connectivity_Recent_Data.csv")
# Install and load required packages
# Define the list of required packages
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
library(readr)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- read.csv("C:/Users/atomi/Documents/GitHub/Team-7_Waterfowl/R_Code/Mallard Connectivity_Recent_Data.csv")
# Reduce the dataset to the first 5 rows
waterfowl_data_reduced <- head(waterfowl_data, 5)
# View the reduced data
print(waterfowl_data_reduced)
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
library(readr)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
# Load the original dataset (if necessary)
waterfowl_data <- read.csv("C:/Users/atomi/Documents/GitHub/Team-7_Waterfowl/R_Code/Mallard Connectivity_Recent_Data.csv")
# Reduce the dataset to the first 5 rows for testing
waterfowl_data <- head(waterfowl_data, 5)
# Step 2: Inspect Dataset Structure
str(waterfowl_data)
head(waterfowl_data)
# Ensure required columns are present
if (!all(c("location.long", "location.lat", "event.id") %in% colnames(waterfowl_data))) {
stop("Dataset must contain 'location-long', 'location-lat', and 'event-id' columns!")
}
# Step 3: Create Spatial Data
sp_data <- st_as_sf(
waterfowl_data,
coords = c("location.long", "location.lat"),
crs = 4326  # WGS84 CRS
)
# Step 4: Create Raster from Observed Data
r <- rast(ext(sp_data), resolution = 0.1)  # Adjust resolution as needed
observed_raster <- rasterize(
sp_data,
r,
field = "event.id",
fun = mean
)
# Step 5: Prepare Data for Prediction
# Normalize observed data
waterfowl_data$event_id_normalized <- scale(waterfowl_data$`event.id`, center = TRUE, scale = TRUE)
set.seed(123)
model <- train(
event_id_normalized ~ `location.long` + `location.lat`,
data = waterfowl_data,
method = "rf"
)
warnings()
# Step 7: Predict Future Locations
predicted_data <- predict(model, newdata = waterfowl_data)
sp_data$predicted <- predicted_data
# Create Predicted Raster
predicted_raster <- rasterize(
sp_data,
r,
field = "predicted",
fun = mean
)
# Step 8: Overlay Current vs. Predicted Data
overlay_raster <- stack(observed_raster, predicted_raster)
# Install and load required packages
# Define the list of required packages
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
library(readr)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
# Load the original dataset (if necessary)
waterfowl_data <- read.csv("C:/Users/atomi/Documents/GitHub/Team-7_Waterfowl/R_Code/Mallard Connectivity_Recent_Data.csv")
# Reduce the dataset to the first 100 rows for testing
waterfowl_data <- head(waterfowl_data, 100)
# Step 2: Inspect Dataset Structure
str(waterfowl_data)
head(waterfowl_data)
# Ensure required columns are present
if (!all(c("location.long", "location.lat", "event.id") %in% colnames(waterfowl_data))) {
stop("Dataset must contain 'location-long', 'location-lat', and 'event-id' columns!")
}
# Step 3: Create Spatial Data
sp_data <- st_as_sf(
waterfowl_data,
coords = c("location.long", "location.lat"),
crs = 4326  # WGS84 CRS
)
# Step 4: Create Raster from Observed Data
r <- rast(ext(sp_data), resolution = 0.1)  # Adjust resolution as needed
observed_raster <- rasterize(
sp_data,
r,
field = "event.id",
fun = mean
)
# Step 5: Prepare Data for Prediction
# Normalize observed data
waterfowl_data$event_id_normalized <- scale(waterfowl_data$`event.id`, center = TRUE, scale = TRUE)
# Step 6: Train Predictive Model
set.seed(123)
model <- train(
event_id_normalized ~ `location.long` + `location.lat`,
data = waterfowl_data,
method = "rf"
)
# Step 7: Predict Future Locations
predicted_data <- predict(model, newdata = waterfowl_data)
sp_data$predicted <- predicted_data
predicted_raster <- rasterize(
sp_data,
r,
field = "predicted",
fun = mean
)
overlay_raster <- stack(observed_raster, predicted_raster)
print(observed_raster)
print(predicted_raster)
print(extent(observed_raster))
print(ext(observed_raster))
print(ext(predicted_raster))
# Step 8: Overlay Current vs. Predicted Data
overlay_raster <- stack(observed_raster, predicted_raster)
# Install and load required packages
# Define the list of required packages
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- read.csv("Mallard Connectivity_Recent_Data.csv")
setwd("~/GitHub/Team-7_Waterfowl/R_Code")
# Install and load required packages
# Define the list of required packages
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- read.csv("Mallard Connectivity_Recent_Data.csv")
waterfowl_data <- head(waterfowl_data, 100)
# Step 2: Inspect Dataset Structure
str(waterfowl_data)
head(waterfowl_data)
# Ensure required columns are present
if (!all(c("location.long", "location.lat", "event.id") %in% colnames(waterfowl_data))) {
stop("Dataset must contain 'location-long', 'location-lat', and 'event-id' columns!")
}
# Step 3: Create Spatial Data
sp_data <- st_as_sf(
waterfowl_data,
coords = c("location.long", "location.lat"),
crs = 4326  # WGS84 CRS
)
# Step 4: Create Raster from Observed Data
r <- rast(ext(sp_data), resolution = 0.1)  # Adjust resolution as needed
observed_raster <- rasterize(
sp_data,
r,
field = "event.id",
fun = mean
)
# Step 5: Prepare Data for Prediction
# Normalize observed data
waterfowl_data$event_id_normalized <- scale(waterfowl_data$`event.id`, center = TRUE, scale = TRUE)
# Step 6: Train Predictive Model
set.seed(123)
model <- train(
event_id_normalized ~ `location.long` + `location.lat`,
data = waterfowl_data,
method = "rf"
)
# Step 7: Predict Future Locations
predicted_data <- predict(model, newdata = waterfowl_data)
sp_data$predicted <- predicted_data
# Create Predicted Raster
predicted_raster <- rasterize(
sp_data,
r,
field = "predicted",
fun = mean
)
# Step 8: Overlay Current vs. Predicted Data
overlay_raster <- stack(observed_raster, predicted_raster)
# Install and load required packages
# Define the list of required packages
# Load libraries
library(sp)
library(terra)
library(sf)
library(caret)
library(fields)
library(rnaturalearth)
library(ggplot2)
# Step 1: Load Dataset
# Assuming the dataset is already in your environment
waterfowl_data <- read.csv("Mallard Connectivity_Recent_Data.csv")
waterfowl_data <- head(waterfowl_data, 500)
# Step 2: Inspect Dataset Structure
str(waterfowl_data)
head(waterfowl_data)
# Ensure required columns are present
if (!all(c("location.long", "location.lat", "event.id") %in% colnames(waterfowl_data))) {
stop("Dataset must contain 'location-long', 'location-lat', and 'event-id' columns!")
}
# Step 3: Create Spatial Data
sp_data <- st_as_sf(
waterfowl_data,
coords = c("location.long", "location.lat"),
crs = 4326  # WGS84 CRS
)
# Step 4: Create Raster from Observed Data
r <- rast(ext(sp_data), resolution = 0.1)  # Adjust resolution as needed
observed_raster <- rasterize(
sp_data,
r,
field = "event.id",
fun = mean
)
# Step 5: Prepare Data for Prediction
# Normalize observed data
waterfowl_data$event_id_normalized <- scale(waterfowl_data$`event.id`, center = TRUE, scale = TRUE)
# Step 6: Train Predictive Model
set.seed(123)
model <- train(
event_id_normalized ~ `location.long` + `location.lat`,
data = waterfowl_data,
method = "rf"
)
# Step 7: Predict Future Locations
predicted_data <- predict(model, newdata = waterfowl_data)
sp_data$predicted <- predicted_data
# Create Predicted Raster
predicted_raster <- rasterize(
sp_data,
r,
field = "predicted",
fun = mean
)
# Step 8: Overlay Current vs. Predicted Data
overlay_raster <- stack(observed_raster, predicted_raster)
install.packages("httr")
library(httr)
library(jsonlite)
# Step 1: Define latitude and longitude
latitude <- 38.8977  # Example: White House
longitude <- -77.0365
# Step 2: Get forecast metadata
url <- paste0("https://api.weather.gov/points/", latitude, ",", longitude)
response <- GET(url, add_headers(`User-Agent` = "MyWeatherApp/1.0 (atomickwnny17@gmail.com)"))
data <- fromJSON(content(response, "text"))
# Extract forecast URL
forecast_url <- data$properties$forecast
# Step 3: Get the forecast data
forecast_response <- GET(forecast_url, add_headers(`User-Agent` = "MyWeatherApp/1.0 (atomickwnny17@gmail.com)"))
forecast_data <- fromJSON(content(forecast_response, "text"))
# Print the forecast
for (i in seq_along(forecast_data$properties$periods)) {
period <- forecast_data$properties$periods[[i]]
cat(paste0(period$name, ": ", period$detailedForecast, "\n"))
}
str(forecast_data)
library(httr)
library(jsonlite)
# Step 1: Define latitude and longitude
latitude <- 38.8977  # Example: White House
longitude <- -77.0365
# Step 2: Get forecast metadata
url <- paste0("https://api.weather.gov/points/", latitude, ",", longitude)
response <- GET(url, add_headers(`User-Agent` = "MyWeatherApp/1.0 (atomickwnny17@gmail.com)"))
data <- fromJSON(content(response, "text"))
# Debug: Print the structure of the response
cat("Metadata Response Structure:\n")
str(data)
# Extract forecast URL
forecast_url <- data$properties$forecast
cat("Forecast URL: ", forecast_url, "\n")
# Step 3: Get the forecast data
if (!is.null(forecast_url)) {
forecast_response <- GET(forecast_url, add_headers(`User-Agent` = "MyWeatherApp/1.0 (atomickwnny17@gmail.com)"))
forecast_data <- fromJSON(content(forecast_response, "text"))
# Debug: Print the structure of the forecast response
cat("Forecast Data Structure:\n")
str(forecast_data)
# Print the forecast
if (!is.null(forecast_data$properties$periods)) {
for (i in seq_along(forecast_data$properties$periods)) {
period <- forecast_data$properties$periods[[i]]
if (is.list(period)) {
cat(paste0(period$name, ": ", period$detailedForecast, "\n"))
}
}
} else {
cat("No forecast data found.\n")
}
} else {
cat("Forecast URL is NULL.\n")
}
install.packages(c("httr", "jsonlite", "dplyr", "purrr"))
library(httr)
library(jsonlite)
library(dplyr)
# Define User-Agent header for the API requests
user_agent_header <- add_headers(`User-Agent` = "MyWeatherApp/1.0 (your_email@example.com)")
# Function to get observation data for a given latitude and longitude
get_weather_observations <- function(latitude, longitude, location_name) {
# Get the metadata URL
url <- paste0("https://api.weather.gov/points/", latitude, ",", longitude)
response <- GET(url, user_agent_header)
if (status_code(response) != 200) {
stop("Error fetching metadata for ", location_name)
}
data <- fromJSON(content(response, "text"), flatten = TRUE)
# Extract observation URL
observation_url <- data$properties.observationStations
# Fetch observations
observations_response <- GET(observation_url, user_agent_header)
if (status_code(observations_response) != 200) {
stop("Error fetching observations for ", location_name)
}
observations_data <- fromJSON(content(observations_response, "text"), flatten = TRUE)
# Extract the latest observation station
station_url <- observations_data$features[[1]]$id
# Fetch data from the station
station_response <- GET(paste0(station_url, "/observations"), user_agent_header)
if (status_code(station_response) != 200) {
stop("Error fetching station data for ", location_name)
}
station_data <- fromJSON(content(station_response, "text"), flatten = TRUE)
# Extract relevant data
observations <- station_data$features %>%
purrr::map_df(~ data.frame(
time = .x$properties.timestamp,
temperature = .x$properties.temperature$value
))
# Convert temperature from Celsius to Fahrenheit and format as a table
observations <- observations %>%
filter(!is.na(temperature)) %>%
mutate(
temperature_F = round((temperature * 9/5) + 32, 1),
location = location_name
) %>%
select(time, temperature_F, location) %>%
arrange(desc(time)) %>%
head(10)
return(observations)
}
# Fetch the last 10 temperatures for Chicago and New York
chicago_data <- get_weather_observations(41.8781, -87.6298, "Chicago")
library(httr)
library(jsonlite)
library(dplyr)
# Define User-Agent header for the API requests
user_agent_header <- add_headers(`User-Agent` = "MyWeatherApp/1.0 (atomickwnny17@gmail.com)")
# Function to get observation data for a given latitude and longitude
get_weather_observations <- function(latitude, longitude, location_name) {
# Get the metadata URL
url <- paste0("https://api.weather.gov/points/", latitude, ",", longitude)
response <- GET(url, user_agent_header)
if (status_code(response) != 200) {
stop("Error fetching metadata for ", location_name)
}
data <- fromJSON(content(response, "text"), flatten = TRUE)
# Extract observation URL
observation_url <- data$properties.observationStations
# Fetch observations
observations_response <- GET(observation_url, user_agent_header)
if (status_code(observations_response) != 200) {
stop("Error fetching observations for ", location_name)
}
observations_data <- fromJSON(content(observations_response, "text"), flatten = TRUE)
# Extract the latest observation station
station_url <- observations_data$features[[1]]$id
# Fetch data from the station
station_response <- GET(paste0(station_url, "/observations"), user_agent_header)
if (status_code(station_response) != 200) {
stop("Error fetching station data for ", location_name)
}
station_data <- fromJSON(content(station_response, "text"), flatten = TRUE)
# Extract relevant data
observations <- station_data$features %>%
purrr::map_df(~ data.frame(
time = .x$properties.timestamp,
temperature = .x$properties.temperature$value
))
# Convert temperature from Celsius to Fahrenheit and format as a table
observations <- observations %>%
filter(!is.na(temperature)) %>%
mutate(
temperature_F = round((temperature * 9/5) + 32, 1),
location = location_name
) %>%
select(time, temperature_F, location) %>%
arrange(desc(time)) %>%
head(10)
return(observations)
}
# Fetch the last 10 temperatures for Chicago and New York
chicago_data <- get_weather_observations(41.8781, -87.6298, "Chicago")
